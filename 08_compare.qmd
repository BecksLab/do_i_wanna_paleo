---
title: "Comparing different methods"
date: last-modified
format:
    html:
        embed-resources: true
title-block-banner: true
bibliography: references.bib
---

-   We can use the networks that we built from the previous section and showcase how they (maybe) are giving way different metrics (telling different stories)

-   I think if we have multiple networks for different time points as well we can compare within and between network differences? (need to have a think on that)

```{r}
#| echo: false
#| warning: false

library(dplyr)
library(ggradar)
library(patchwork)
library(readr)
library(scales)
library(tidyverse)

df = list.files(path="data/processed/", full.names = TRUE) %>% 
  lapply(read_csv) %>% 
  bind_rows %>% 
        # to get the ratio
        mutate(ratio = top/basal,
                top = NULL,
                basal = NULL) %>%
        pivot_longer(
            cols = -c(id, model), 
            names_to = "stat",
            values_to = "stat_val")  %>% 
    # standardise names
        mutate(id = case_when(
            str_detect(id, "^.*pre.*$") ~ "pre",
            str_detect(id, "^.*during.*$") ~ "post",
            str_detect(id, "^.*post.*$") ~ "during",
            TRUE ~ as.character(id)))

df$id <- ordered(df$id, levels=c("pre", "during", "post"))

```

```{r}
#| echo: false
#| label: fig-line
#| fig-cap: "Comparing the different structures that are recovered by the different models (colours) over a time series. We can ignore richness - that's more a sanity check for me. S1, S2, S4, S5 are motifs; S1 = Number of linear chains, S2 = Number of omnivory motifs, S4 = Number of apparent competition motifs, S5 = Number of direct competition motifs"


ggplot(df,
        aes(x = factor(`id`), 
            y = stat_val, 
            colour = model,
            group = model)) +
    geom_line() +
    geom_point() +
    facet_wrap(vars(stat),
                scales = 'free') +
    scale_size(guide = 'none') +
    theme_classic() +
    xlab("time") +
    ylab("value") +
    theme(panel.border = element_rect(colour = 'black',
                                      fill = "#ffffff00"),
            axis.ticks.x = element_blank())

```

We can obviously look at different 'summary statistics' (see [Form v Function](10_structure_vs_form.qmd)) but just throwing these out for now...

Similar signal but different values... 

Also the challenge that we don't really know what is 'true' so don't actually know how much we can infer but it is interesting to see what the different models are spitting out even if they are potentially wrong... 

Also also does this influence the way we are thinking about the idea of 'feasibility' vs 'energy' networks? One would think they might have clustered a bit differently... although arguably the PFIM is the only feasibility network here and it is often the (visual) outlier...